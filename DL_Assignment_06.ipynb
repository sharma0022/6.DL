{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhIU4MowbPT2GOaV9bKYfG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/DL_Theory/blob/main/DL_Assignment_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tWhat are the advantages of a CNN over a fully connected DNN for image classification?**\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "* CNN has fewer parameters,so faster to train.It uses Reuse kernel which can detect feature anywhere.\n",
        "\n",
        "* Architecture embeds knowledge of neighboring pixels.\n",
        "\n",
        "* Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
        "\n",
        "* When a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.\n",
        "\n",
        "* Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN's architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs."
      ],
      "metadata": {
        "id": "YDpvyhxTjpfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.**\n",
        "**What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?**\n",
        "\n",
        "**Ans:**\n",
        "* first convolutional layer: 3 * 3 * 3 + 1 = 28 output feature maps is 100: 28 100 = 2800\n",
        "\n",
        "* second convolutional layer kernel-size and last feature maps, plus bias: 3 * 3  * 100 + 1 = 901 output feature maps is 200: 901 * 200 = 180200\n",
        "\n",
        "* third convolutional layers kernel-size and last feautre maps, plus bias: 3 * 3 * 200 + 1 =1801 output feautre maps is 400: 1801 * 400 = 720400\n",
        "\n",
        "* Total parameters is 2800 + 180200 + 720400 = 903400\n",
        "\n",
        "* 903400 * 4 / 1024 / 1024 = **3.44 (MB)** \n",
        "\n",
        "**memories since 32-bit is 4 bytes**\n",
        "\n",
        "* first convolutional layer one feature map size: 100 * 150 = 15000 total output: 15000 * 100 = 1,500,000 \n",
        "\n",
        "* second convolutional layer one feature map size: 50 * 75 = 3,750 total output: 3750 * 200 = 750,000\n",
        "\n",
        "* third convolutional layer one feature map size: 25 * 38 = 950 total ouput: 950 * 400 = 380, 000\n",
        "\n",
        "* (1,500,000 + 750,000 + 380,000) * 4 / 1024 /1024 = **10.032 (MB)**\n",
        "\n",
        "* Total: 10.032+ 3.44=13.47(MB)"
      ],
      "metadata": {
        "id": "WMP3vEBEjoIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?**\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "* reduce mini-batch size\n",
        "* reduce dimensionality using a larger stride in one or more layers\n",
        "* remove one or more layers\n",
        "* using 16-bits instead of 32-bit floats\n",
        "*distributed the cnn across multiple devices"
      ],
      "metadata": {
        "id": "uHqp0qfyjlwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?**\n",
        "\n",
        "**Ans:** Max pooling is a type of operation that is typically added to CNNs following individual convolutional layers. When added to a model, max pooling reduces the dimensionality of images by reducing the number of pixels in the output from the previous convolutional layer"
      ],
      "metadata": {
        "id": "rmF09_YOjkNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.\tWhen would you want to add a local response normalization layer?**\n",
        "\n",
        "**Ans:**This layer is useful when we are dealing with ReLU neurons.Because ReLU neurons have unbounded activations and we need LRN to normalize that."
      ],
      "metadata": {
        "id": "XeXBtqbcjilw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?**\n",
        "\n",
        "**Ans:**\n",
        "* AlexNet : It is much larger and deeper stacks convolutional layer directly on top of each convlutional layer.\n",
        "\n",
        "* GooLeNet: Introduce a inception modules, which make it possible to have much deeper net than previous network.\n",
        "\n",
        "* ResNet: Introduce a skip connection.\n"
      ],
      "metadata": {
        "id": "BYxWTXVZjgn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?**\n",
        "\n",
        "**Ans:** Fully Convolutional Networks are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as convolution, pooling and upsampling. Avoiding the use of dense layers means less parameters (making the networks faster to train).\n",
        "\n",
        "If we set the kernel size to match the input size. Setting the number of filters is then the same as setting the number of output neurons in a fully connected layer."
      ],
      "metadata": {
        "id": "HTsMgAQ1jfE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.\tWhat is the main technical difficulty of semantic segmentation?**\n",
        "\n",
        "**Ans:** \n",
        "* It can only identify a single type of defect  \n",
        "* It lack of pixel-level segmentation information from the model "
      ],
      "metadata": {
        "id": "I1HMt9TIjbKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.**"
      ],
      "metadata": {
        "id": "dSKOaq3ijZkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jgnFz62ATDRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(xtrain,ytrain),(xtest,ytest) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgveXtHaWDIX",
        "outputId": "dfd04c83-2ec9-4204-cf05-075c0c69cc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(xtrain[4])\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "hBgmtfppcbyv",
        "outputId": "d185fbeb-b3bc-44d4-e9e0-3c532c4f5a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(*args, **kw)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain=np.reshape(xtrain,(-1,28,28,1))\n",
        "xtest=np.reshape(xtest,(-1,28,28,1))\n",
        "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVQb0R0Rfl_X",
        "outputId": "a5b9cda8-2cc7-40b9-fdcc-1ff1650718c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (10000, 28, 28, 1), (60000,), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain=xtrain/255\n",
        "xtest=xtest/255"
      ],
      "metadata": {
        "id": "4hIak7W2fqLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "y_train = to_categorical(ytrain, num_classes=10)\n",
        "y_test = to_categorical(ytest, num_classes=10)"
      ],
      "metadata": {
        "id": "D7VDbq1Cft1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "model=Sequential()\n",
        "     \n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
        "     \n",
        "model.add(Conv2D(32,kernel_size=5,strides=1,padding=\"Same\",activation=\"relu\",input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(padding=\"same\"))\n",
        "\n",
        "model.add(Conv2D(64,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"))\n",
        "model.add(MaxPooling2D(padding=\"same\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1024,activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation=\"sigmoid\"))\n",
        "     \n",
        "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "     \n",
        "model.fit(xtrain,y_train,batch_size=100,epochs=5,validation_data=(xtest,y_test))    "
      ],
      "metadata": {
        "id": "H9Au3elNf0Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80d299a-a560-4150-a673-42eb0cc15fca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "600/600 [==============================] - 242s 396ms/step - loss: 0.1298 - accuracy: 0.9597 - val_loss: 0.0362 - val_accuracy: 0.9886\n",
            "Epoch 2/5\n",
            "600/600 [==============================] - 240s 400ms/step - loss: 0.0381 - accuracy: 0.9886 - val_loss: 0.0249 - val_accuracy: 0.9910\n",
            "Epoch 3/5\n",
            "600/600 [==============================] - 226s 376ms/step - loss: 0.0270 - accuracy: 0.9914 - val_loss: 0.0244 - val_accuracy: 0.9912\n",
            "Epoch 4/5\n",
            "600/600 [==============================] - 220s 366ms/step - loss: 0.0184 - accuracy: 0.9940 - val_loss: 0.0287 - val_accuracy: 0.9900\n",
            "Epoch 5/5\n",
            "600/600 [==============================] - 223s 372ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0257 - val_accuracy: 0.9918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1aac509dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(xtrain,y_train),model.evaluate(xtest,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho3L1sV-hNTG",
        "outputId": "71d69536-787f-4108-fd5c-41b5a4c2445c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 75s 40ms/step - loss: 0.0106 - accuracy: 0.9967\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 0.0257 - accuracy: 0.9918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.010635819286108017, 0.9966833591461182],\n",
              " [0.025650884956121445, 0.9918000102043152])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}